{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference - this code is just modification of https://github.com/dennybritz/reinforcement-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run ../utils/commonImports.py\n",
    "%run ../utils/tradingImports.py\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import itertools\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import namedtuple\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "EpisodeStats = namedtuple(\"Stats\",[\"episode_lengths\", \"episode_rewards\"])\n",
    "\n",
    "def plot_cost_to_go_mountain_car(env, estimator, num_tiles=20):\n",
    "    x = np.linspace(env.observation_space.low[0], env.observation_space.high[0], num=num_tiles)\n",
    "    y = np.linspace(env.observation_space.low[1], env.observation_space.high[1], num=num_tiles)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = np.apply_along_axis(lambda _: -np.max(estimator.predict(_)), 2, np.dstack([X, Y]))\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1,\n",
    "                           cmap=matplotlib.cm.coolwarm, vmin=-1.0, vmax=1.0)\n",
    "    ax.set_xlabel('Position')\n",
    "    ax.set_ylabel('Velocity')\n",
    "    ax.set_zlabel('Value')\n",
    "    ax.set_title(\"Mountain \\\"Cost To Go\\\" Function\")\n",
    "    fig.colorbar(surf)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_value_function(V, title=\"Value Function\"):\n",
    "    \"\"\"\n",
    "    Plots the value function as a surface plot.\n",
    "    \"\"\"\n",
    "    min_x = min(k[0] for k in V.keys())\n",
    "    max_x = max(k[0] for k in V.keys())\n",
    "    min_y = min(k[1] for k in V.keys())\n",
    "    max_y = max(k[1] for k in V.keys())\n",
    "\n",
    "    x_range = np.arange(min_x, max_x + 1)\n",
    "    y_range = np.arange(min_y, max_y + 1)\n",
    "    X, Y = np.meshgrid(x_range, y_range)\n",
    "\n",
    "    # Find value for all (x, y) coordinates\n",
    "    Z_noace = np.apply_along_axis(lambda _: V[(_[0], _[1], False)], 2, np.dstack([X, Y]))\n",
    "    Z_ace = np.apply_along_axis(lambda _: V[(_[0], _[1], True)], 2, np.dstack([X, Y]))\n",
    "\n",
    "    def plot_surface(X, Y, Z, title):\n",
    "        fig = plt.figure(figsize=(20, 10))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1,\n",
    "                               cmap=matplotlib.cm.coolwarm, vmin=-1.0, vmax=1.0)\n",
    "        ax.set_xlabel('Player Sum')\n",
    "        ax.set_ylabel('Dealer Showing')\n",
    "        ax.set_zlabel('Value')\n",
    "        ax.set_title(title)\n",
    "        ax.view_init(ax.elev, -120)\n",
    "        fig.colorbar(surf)\n",
    "        plt.show()\n",
    "\n",
    "    plot_surface(X, Y, Z_noace, \"{} (No Usable Ace)\".format(title))\n",
    "    plot_surface(X, Y, Z_ace, \"{} (Usable Ace)\".format(title))\n",
    "\n",
    "\n",
    "\n",
    "def plot_episode_stats(stats, smoothing_window=10, noshow=False):\n",
    "    # Plot the episode length over time\n",
    "    fig1 = plt.figure(figsize=(10,5))\n",
    "    plt.plot(stats.episode_lengths)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Episode Length\")\n",
    "    plt.title(\"Episode Length over Time\")\n",
    "    if noshow:\n",
    "        plt.close(fig1)\n",
    "    else:\n",
    "        plt.show(fig1)\n",
    "\n",
    "    # Plot the episode reward over time\n",
    "    fig2 = plt.figure(figsize=(10,5))\n",
    "    rewards_smoothed = pd.Series(stats.episode_rewards).rolling(smoothing_window, min_periods=smoothing_window).mean()\n",
    "    plt.plot(rewards_smoothed)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Episode Reward (Smoothed)\")\n",
    "    plt.title(\"Episode Reward over Time (Smoothed over window size {})\".format(smoothing_window))\n",
    "    if noshow:\n",
    "        plt.close(fig2)\n",
    "    else:\n",
    "        plt.show(fig2)\n",
    "\n",
    "    # Plot time steps and episode number\n",
    "    fig3 = plt.figure(figsize=(10,5))\n",
    "    plt.plot(np.cumsum(stats.episode_lengths), np.arange(len(stats.episode_lengths)))\n",
    "    plt.xlabel(\"Time Steps\")\n",
    "    plt.ylabel(\"Episode\")\n",
    "    plt.title(\"Episode per time step\")\n",
    "    if noshow:\n",
    "        plt.close(fig3)\n",
    "    else:\n",
    "        plt.show(fig3)\n",
    "\n",
    "    return fig1, fig2, fig3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataRoot = 'D:\\\\Dropbox\\\\My work\\\\krypl-project\\\\dataLabeled\\\\poloniex\\\\5min'\n",
    "fileName = 'USDT_BTC_5min_2016-01-01_2017-12-31.tsv'\n",
    "file = '{dataRoot}\\\\{fileName}'.format(dataRoot=dataRoot, fileName=fileName)\n",
    "\n",
    "usdtBtc = pd.read_csv(file, sep='\\t').query(\"date >= '2017-01-01'\")\\\n",
    "    .sort_values('timestamp').reset_index().drop('index', axis=1)\n",
    "usdtBtc = usdtBtc\n",
    "\n",
    "trainRatio = 0.7\n",
    "trainSize = int(usdtBtc.shape[0] * trainRatio)\n",
    "usdtBtcTrain = usdtBtc.iloc[:trainSize]\n",
    "usdtBtcTest = usdtBtc.iloc[trainSize:].reset_index().drop('index', axis=1)\n",
    "\n",
    "usdtBtcManagerTrain = CurrencyDataManager(usdtBtcTrain['close'], usdtBtcTrain)\n",
    "usdtBtcManagerTest = CurrencyDataManager(usdtBtcTest['close'], usdtBtcTest)\n",
    "wallet = {'usdt': 1000}\n",
    "contractPair = ContractPair('usdt', 'btc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataManager = deepcopy(usdtBtcManagerTrain)\n",
    "exchange = BackTestExchange(dataManager, deepcopy(wallet), 0.0025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy Estimators\n",
    "\n",
    "Just to test learning performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DummyPolicyEstimator:\n",
    "    def predict(self, state):\n",
    "        return [0.7, 0.1, 0.2] \n",
    "    \n",
    "    def update(self, state, target, action):\n",
    "        pass\n",
    "\n",
    "    \n",
    "class DummyValueEstimator:\n",
    "    def predict(self, state):\n",
    "        return 0\n",
    "    \n",
    "    def update(self, state, target):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling.rl.enviroment import ExchangeEnv\n",
    "from modeling.rl.estimator import PerceptronPolicyEstimator, PerceptronValueEstimator\n",
    "\n",
    "cols = ['close', 'timestamp', 'high', 'low', 'open', 'quoteVolume', 'volume']\n",
    "\n",
    "env = ExchangeEnv(\n",
    "    data=usdtBtcTrain[cols],\n",
    "    priceCol='close',\n",
    "    contractPair=contractPair,\n",
    "    wallet=wallet,\n",
    "    fee=0.0025,\n",
    "    epochLen=1440,\n",
    "    buyAmount=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getState(envState):\n",
    "    return np.array([envState])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reinforce(env, estimator_policy, estimator_value, num_episodes, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    REINFORCE (Monte Carlo Policy Gradient) Algorithm. Optimizes the policy\n",
    "    function approximator using policy gradient.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        estimator_policy: Policy Function to be optimized \n",
    "        estimator_value: Value function approximator, used as a baseline\n",
    "        num_episodes: Number of episodes to run for\n",
    "        discount_factor: Time-discount factor\n",
    "    \n",
    "    Returns:\n",
    "        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "\n",
    "    # Keeps track of useful statistics\n",
    "    stats = EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))    \n",
    "    \n",
    "    Transition = collections.namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        # Reset the environment and pick the fisrst action\n",
    "        state = getState(env.reset())\n",
    "        episode = []\n",
    "        \n",
    "        # One step in the environment\n",
    "        for t in itertools.count():\n",
    "            \n",
    "            # Take a step\n",
    "            action_probs = estimator_policy.predict(state)\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = getState(next_state)\n",
    "            # Keep track of the transition\n",
    "            episode.append(Transition(state=state, action=action, reward=reward, next_state=next_state, done=done))\n",
    "            \n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "            \n",
    "            # Print out which step we're on, useful for debugging.\n",
    "            print(\"\\rStep {} @ Episode {}/{} ({})\".format(\n",
    "                    t, i_episode + 1, num_episodes, stats.episode_rewards[i_episode - 1]), end=\"\")\n",
    "            # sys.stdout.flush()\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "    \n",
    "        # Go through the episode and make policy updates\n",
    "        for t, transition in enumerate(episode):\n",
    "            # The return after this timestep\n",
    "            total_return = sum(discount_factor**i * t.reward for i, t in enumerate(episode[t:]))\n",
    "            # Calculate baseline/advantage\n",
    "            baseline_value = estimator_value.predict(transition.state)            \n",
    "            advantage = total_return - baseline_value\n",
    "            # Update our value estimator\n",
    "            estimator_value.update(transition.state, total_return)\n",
    "            # Update our policy estimator\n",
    "            estimator_policy.update(transition.state, advantage, transition.action)\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile, pstats, io\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "\n",
    "\n",
    "# ---- START -----\n",
    "\n",
    "\n",
    "# tf.reset_default_graph()\n",
    "\n",
    "# global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "# policy_estimator = PerceptronPolicyEstimator(env.observation_space.n, env.action_space.n)\n",
    "# value_estimator = PerceptronValueEstimator(env.observation_space.n)\n",
    "\n",
    "policy_estimator = DummyPolicyEstimator()\n",
    "value_estimator = DummyValueEstimator()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    stats = reinforce(env, policy_estimator, value_estimator, 2, discount_factor=1.0)\n",
    "    \n",
    "# ---- END ----\n",
    "\n",
    "pr.disable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = io.StringIO()\n",
    "sortby = 'tottime'\n",
    "ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n",
    "ps.print_stats()\n",
    "print(s.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def filterEmpty(ll):\n",
    "    return list(filter(lambda x: len(x)!=0, ll))\n",
    "\n",
    "def statsToPandas(stats):\n",
    "    lines = filterEmpty(stats.getvalue().split('\\n'))\n",
    "    cols = filterEmpty(lines[2].split(' '))\n",
    "    rows = [filterEmpty(l.split(' ')) for l in lines[3:]]\n",
    "    rows = [l[:5] + [''.join(l[5:])] for l in rows]\n",
    "    df = pd.DataFrame(rows, columns=cols)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = statsToPandas(s)\n",
    "kryplCalls = df[df['filename:lineno(function)'].str.contains('krypl-project')]\n",
    "kryplCalls['file'] = kryplCalls['filename:lineno(function)'].apply(lambda x: x.replace('D:\\\\Gitprojekty\\\\krypl-project\\\\', ''))\\\n",
    "    .apply(lambda x: x.replace('D:\\\\GitProjects\\\\krypl-project\\\\', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kryplCalls.sort_values('tottime', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_episode_stats(stats, smoothing_window=25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
